{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "from selenium import webdriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cpso.on.ca/Public-Information-Services/Find-a-Doctor?search=general'\n",
    "\n",
    "# Load the list of all 3 letter postal codes in ontario\n",
    "postalCodes = pd.read_csv('./res/ON Postal Code.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the profile url for future use\n",
    "def getProfile(element):\n",
    "    link = element.find_element_by_tag_name('a').get_attribute('href')\n",
    "    return link\n",
    "\n",
    "\n",
    "# Gets the name from the page\n",
    "def getName(element):\n",
    "    name = element.find_element_by_tag_name('h3').text\n",
    "    return name\n",
    "\n",
    "\n",
    "# Gets the location info\n",
    "def getLocation(element):\n",
    "    location = element.find_element_by_tag_name('p').text\n",
    "    return location\n",
    "\n",
    "\n",
    "# gets the specialty if its listed else make it blank\n",
    "def getSpecialty(element):\n",
    "    try:\n",
    "        specialty = element.find_elements_by_tag_name('div')[-1].find_element_by_tag_name('p').text\n",
    "    except:\n",
    "        specialty = \"\"\n",
    "    return specialty\n",
    "\n",
    "\n",
    "# Append the results to the file\n",
    "def appendToFile(filePath, df):\n",
    "    header = True\n",
    "    \n",
    "    # if file exists then add header\n",
    "    if os.path.isfile(filePath):\n",
    "        header = False\n",
    "        \n",
    "    df.to_csv(filePath, mode='a', header=header, index=False)\n",
    "    \n",
    "# Gets all the elements that are contain the data        \n",
    "def getScrapePageInfo():\n",
    "    articleList = browser.find_elements_by_tag_name('article')\n",
    "    profileList = list(map(getProfile, articleList))\n",
    "    nameList = list(map(getName, articleList))\n",
    "    locationList = list(map(getLocation, articleList))\n",
    "    specialty = list(map(getSpecialty, articleList))\n",
    "    # Save the data in a dataFrame\n",
    "    data = pd.DataFrame(columns={'Name', 'Link', 'Location', 'Specialty'})\n",
    "    data['Name'] = nameList\n",
    "    data['Link'] = profileList\n",
    "    data['Location'] = locationList\n",
    "    data['Specialty'] = specialty\n",
    "    return data\n",
    "    \n",
    "    \n",
    "# Loop through the pages after search and scrape  info\n",
    "def scrapePages():\n",
    "    \n",
    "    # Gets the page limit\n",
    "    pageLimit = browser.find_element_by_css_selector('.row.doctor-search-count').find_element_by_css_selector('.medium-4.columns.text-align--right').text\n",
    "    pageLimit = int(pageLimit.split(' ')[-1][:-1])\n",
    "    \n",
    "    # Scroll through the pages\n",
    "    for i in range(1, pageLimit+1):\n",
    "        \n",
    "        # dataframe that equals the scraped page\n",
    "        df = getScrapePageInfo()\n",
    "        \n",
    "        appendToFile('./csv/ON Physician.csv', df)\n",
    "        # Delete the dataframe once its been saved\n",
    "        del df \n",
    "        try:\n",
    "            pageList = browser.find_element_by_css_selector('.doctor-search-paging')\n",
    "            if (i-1) % 5 == 0 and (i-1) != 0:\n",
    "                nextPage = pageList.find_element_by_xpath(\"//a[contains(text(),'Next 5')]\".format(i))\n",
    "            else:\n",
    "                nextPage = pageList.find_element_by_xpath(\"//a[text() = '{}']\".format(i))\n",
    "            nextPage.click()\n",
    "        except:\n",
    "            print('No Pages')\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('-incognito')\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='../chromedriver', chrome_options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each postal code\n",
    "for i in range(len(postalCodes)):\n",
    "    \n",
    "    # Go to advance search page on CPSO\n",
    "    browser.get(url)\n",
    "    \n",
    "    # Wait anyway where from 0.5-2 seconds for the page to load\n",
    "    time.sleep(random.uniform(0.5,2))\n",
    "    \n",
    "    # Grab the postal code search input\n",
    "    inputForm = browser.find_element_by_id('p_lt_ctl04_pageplaceholder_p_lt_ctl02_AllDoctorsSearch_txtPostalCode')\n",
    "    \n",
    "    # Send the ith postal code\n",
    "    inputForm.send_keys(postalCodes.iloc[i, 0])\n",
    "    \n",
    "    # Grab the submit button\n",
    "    submitButton = browser.find_element_by_id('p_lt_ctl04_pageplaceholder_p_lt_ctl02_AllDoctorsSearch_btnSubmit1')\n",
    "    \n",
    "    # Press submit\n",
    "    submitButton.click()\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(random.uniform(0.5,2))\n",
    "    \n",
    "    # Try to scrape the page if there are doctors in that FSA\n",
    "    try:            \n",
    "        scrapePages()\n",
    "    except:\n",
    "        print('No doctors in FSA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
